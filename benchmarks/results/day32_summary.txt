================================================================================
  EVENTSTREAM CORE - DAY 32 SUMMARY
  Performance Profiling & Clock Fix
================================================================================

PART A: Clock Fix
================================================================================
BEFORE (Day 31): high_resolution_clock
- Subject to OS scheduling + NTP adjustments
- Inflates latency measurements

AFTER (Day 32): steady_clock (monotonic)
- Monotonic (always increases)
- Not affected by system time adjustments
- Accurate for latency measurement

Result: Confirmed queue latencies ARE REAL (not measurement artifact)

PART B: Split Latency Measurement  
================================================================================
Now tracking 3 latency components:

1. Queue Wait Latency: ingest_ts → dequeue_ts
   - Time event sits in queue
   - SPSC p99: 672ms

2. Processing Latency: dequeue_ts → process_done_ts
   - Time to process event
   - SPSC p99: 37ns (negligible)

3. End-to-End Latency: ingest_ts → process_done_ts
   - Total latency
   - SPSC p99: 672ms (= queue wait + processing)

Key Finding: Queue latency dominates (672ms vs 37ns processing)

PART C: Perf Profiling Results (SPSC)
================================================================================

Top CPU Consumers:
┌─────────────────────────────────────┬──────────┬─────────────────────────┐
│ Function                            │ % CPUs   │ Root Cause              │
├─────────────────────────────────────┼──────────┼─────────────────────────┤
│ std::mutex::lock()                  │ 21.37%   │ Futex syscall + context │
│ std::vector::push_back()            │ 13.84%   │ Memory allocation/page  │
│ std::mutex::unlock()                │  6.52%   │ Futex wake syscall      │
│ std::queue::front()                 │  5.44%   │ Deque overhead          │
│ Clock::now_ns()                     │  4.99%   │ clock_gettime syscall   │
└─────────────────────────────────────┴──────────┴─────────────────────────┘

Detailed Breakdown:
1. FUTEX CONTENTION (21.37%)
   └─ Entry: std::mutex::lock()
   └─ Path: mutex.lock() → __gthread_mutex_lock → __GI___lll_lock_wait
           → futex syscall → kernel scheduler
   └─ Cost: Context switch (8.30%), schedule overhead
   └─ Why: Consumer sleeps on futex when queue empty, producer must wake

2. VECTOR ALLOCATION (13.84%)
   └─ Entry: vector<uint64_t>::push_back()
   └─ Path: vector capacity exhausted → reallocation → malloc
   └─ Cost: Page fault (6.33%), memory management
   └─ Why: Growing vector without pre-allocation

3. FUTEX WAKE (6.52%)
   └─ Entry: std::mutex::unlock()
   └─ Path: mutex.unlock() → futex wake syscall
   └─ Cost: Kernel scheduler + spin lock (native_queued_spin_lock)

4. DEQUE ACCESS (5.44%)
   └─ std::queue uses std::deque internally
   └─ Deque adds indexing overhead vs simple circular buffer

5. CLOCK SYSCALL (4.99%)
   └─ clock_gettime() to kernel
   └─ Could be optimized with VDSO mapping (if available)

Threads Analysis:
- Consumer: 57.18% (heavily blocked on mutex)
- Producer: 18.00% (also waits for consumer via queue)
- System: ~25% (kernel scheduler, context switches)

Bottleneck Chain:
Producer fills → Queue fills → Consumer blocked on lock → Producer waits
↑_______________________________________________________________↓
This creates 450ms+ latency even though actual work is 37ns

PERFORMANCE BASELINE COMPARISON
================================================================================

Scenario     │ Throughput  │ p50 Queue Wait │ p99 Queue Wait │ Bottleneck
─────────────┼─────────────┼────────────────┼────────────────┼──────────────
SPSC         │ 2.86M eps   │ 394ms          │ 672ms          │ Mutex lock
MPSC         │ 1.94M eps   │ 1.18s          │ 1.72s          │ 1 consumer
MPMC         │ 1.17M eps   │ 1.12s          │ 1.33s          │ Lock contention
─────────────┴─────────────┴────────────────┴────────────────┴──────────────

Insights:
1. SPSC: Still dominated by mutex lock (not contention from multiple users,
   but futex syscall overhead)
2. MPSC: Worst case - single consumer can't keep up with 8 producers
3. MPMC: Better than MPSC but still lock-contended

ROOT CAUSE SUMMARY
================================================================================

The queue latencies are NOT due to:
  ✗ Poor clock measurements
  ✗ System sleep/NTP drift
  ✗ Allocator performance

The queue latencies ARE due to:
  ✓ Mutex lock futex syscalls (21.37% CPU)
  ✓ Context switches during lock wait (8.30% CPU)
  ✓ Memory allocation for metrics (13.84% CPU)
  ✓ Deque overhead (5.44% CPU)

The ACTUAL queue residence time (depth of queue): 2-5 events
The MEASURED end-to-end latency: 450-672ms

Why the huge difference?
→ Producer thread blocks in mutex.lock() waiting for consumer to drain
→ Consumer thread blocks in futex_wait for more events
→ They're synchronizing via expensive kernel syscalls

OPTIMIZATION ROADMAP
================================================================================

Ranked by impact:

1. LOCK-FREE QUEUE (4-5x improvement expected)
   Before: 21.37% in mutex + 6.52% in unlock = 27.89% in locks
   After:  ~3% in atomic operations
   Result: 2.86M → 10-15M events/sec

2. MEMORY PRE-ALLOCATION (2-3x improvement for metrics)
   Before: 13.84% in vector reallocation
   After:  ~1% in push_back
   Result: Smoother allocation pattern

3. LOCK-FREE CLOCK (small)
   Before: 4.99% in clock_gettime syscall
   After:  ~0.5% (with VDSO or inline cycle counter)
   Result: Minor improvement

4. REPLACE DEQUE WITH CIRCULAR BUFFER (small)
   Before: 5.44% deque overhead
   After:  ~1% simple array indexing
   Result: Better cache locality

Next Phase: Implement lock-free queue and retest

================================================================================
Generated: Day 32 Performance Analysis
File: benchmarks/results/day32_summary.txt
================================================================================
