# Core Engine Technical Deep-Dive

**EventStreamCore Architecture & Implementation Details** â€” Lock-free algorithms, NUMA optimization, zero-allocation design, and systems programming techniques.

---

## ğŸ“‘ Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Lock-Free Data Structures](#lock-free-data-structures)
3. [NUMA Optimization](#numa-optimization)
4. [Zero-Allocation Event Pools](#zero-allocation-event-pools)
5. [Binary Frame Protocol](#binary-frame-protocol)
6. [Topic Routing & Dispatch](#topic-routing--dispatch)
7. [Control Plane & Backpressure](#control-plane--backpressure)
8. [Deduplication & Idempotency](#deduplication--idempotency)
9. [Storage & Dead Letter Queue](#storage--dead-letter-queue)
10. [Performance Analysis](#performance-analysis)

---

## Architecture Overview

EventStreamCore follows a **pipeline architecture** optimized for extreme throughput and minimal latency:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingest Layer â”‚  Multi-protocol (TCP/UDP), zero-copy parsing
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Event Pools â”‚  Thread-local, NUMA-aware, pre-allocated objects
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MPSC Queue  â”‚  Lock-free Vyukov algorithm, 64K capacity
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dispatcher  â”‚  Central router, O(1) topic lookup, metrics collection
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EventBusMultiâ”‚  Multi-consumer fan-out via SPSC queues
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Workers    â”‚  ProcessManager orchestrates NUMA-bound worker threads
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component | Purpose | Thread Model | Key Technology |
|-----------|---------|--------------|----------------|
| **TcpIngestServer** | TCP socket ingestion | Multi-threaded (1 accept + N client handlers) | Per-client thread pool |
| **UdpIngestServer** | UDP datagram ingestion | Multi-threaded (N receivers) | `recvmmsg()` batching |
| **FrameParser** | Binary protocol decoder | Stateless (called by ingest) | Zero-copy pointers |
| **IngestEventPool** | Event object pooling | Thread-local storage | NUMA-aware allocation |
| **MPSC Queue** | Ingest â†’ Dispatcher queue | Lock-free multi-producer | Vyukov CAS algorithm |
| **Dispatcher** | Event routing engine | **Single-threaded** (critical!) | TopicTable hash map |
| **TopicTable** | Topic â†’ handler mapping | Read-heavy (shared_mutex) | O(1) hash lookup |
| **ControlPlane** | Adaptive backpressure | Evaluated by Dispatcher | State machine transitions |
| **PipelineStateManager** | Global state coordination | Atomic reads/writes | Lock-free state machine |
| **EventBusMulti** | Worker distribution | Single-producer (Dispatcher) | N SPSC queues |
| **SPSC Queues** | Dispatcher â†’ Worker queues | Lock-free single-producer | Atomic ring buffer |
| **ProcessManager** | Worker orchestration | Multi-threaded (N workers) | NUMA CPU affinity |
| **LockFreeDeduplicator** | Idempotency check | Lock-free reads, CAS writes | Atomic hash map |
| **StorageEngine** | Persistence + DLQ | Background writer | Batch file I/O |
| **MetricRegistry** | Performance monitoring | Thread-safe counters | Mutex-protected map |

### Data Flow Example

**TCP Frame Received** â†’ **TcpIngestServer handler thread**:

1. `recv()` reads binary frame (length-prefixed)
2. `parseFrameBody()` extracts priority, topic, payload (zero-copy)
3. `IngestEventPool::acquireEvent()` gets pre-allocated event object
4. Populate event fields (topic, payload, timestamp)
5. `mpscQueue.tryPush(event)` â†’ lock-free CAS insertion

**Dispatcher thread** (infinite loop):

6. `mpscQueue.tryPop()` â†’ dequeue event (single-threaded, no contention)
7. Check `PipelineStateManager::getState()` (RUNNING? PAUSED? DRAINING?)
8. `TopicTable::getHandler(topic)` â†’ O(1) hash lookup
9. `ControlPlane::evaluateMetrics()` â†’ check thresholds
10. `EventBusMulti::publish(event)` â†’ round-robin to worker SPSC queues
11. `MetricRegistry::incrementCounter()` â†’ track throughput

**Worker thread** (e.g., RealtimeProcessor):

12. `spscQueue.tryPop()` â†’ dequeue from dedicated SPSC queue
13. `processEvent(event)` â†’ application-specific logic
14. `IngestEventPool::releaseEvent(event)` â†’ return to pool (custom deleter)

---

## Lock-Free Data Structures

### MPSC Queue (Multi-Producer Single-Consumer)

**Implementation**: Dmitry Vyukov's bounded MPSC queue  
**Paper**: [Bounded MPSC Queue](http://www.1024cores.net/home/lock-free-algorithms/queues/bounded-mpsc-queue)  
**Code**: [include/utils/mpsc_queue.hpp](../../include/utils/mpsc_queue.hpp)

#### Algorithm Overview

**Core Idea**: Combine atomic operations with single-consumer invariant to eliminate locks.

**Data Structure**:

```cpp
template<typename T>
class MPSCQueue {
private:
    struct alignas(64) Cell {  // Cache-line aligned (prevent false sharing)
        std::atomic<uint64_t> sequence;  // Slot version number
        T data;                          // Event pointer
    };
    
    alignas(64) std::atomic<uint64_t> enqueue_pos_;  // Producer counter (CAS)
    alignas(64) std::atomic<uint64_t> dequeue_pos_;  // Consumer counter (single-writer)
    
    std::vector<Cell> buffer_;  // Power-of-2 size (65536)
    uint64_t mask_;             // capacity - 1 (for fast modulo)
};
```

#### Push Operation (Multi-Producer)

**Lock-Free CAS Loop**:

```cpp
bool tryPush(const T& item) {
    uint64_t pos = enqueue_pos_.load(std::memory_order_relaxed);
    
    for (;;) {  // CAS retry loop
        Cell* cell = &buffer_[pos & mask_];  // Fast modulo via bitwise AND
        uint64_t seq = cell->sequence.load(std::memory_order_acquire);
        intptr_t diff = (intptr_t)seq - (intptr_t)pos;
        
        if (diff == 0) {  // Slot available
            // Try to claim this slot with CAS
            if (enqueue_pos_.compare_exchange_weak(pos, pos + 1,
                    std::memory_order_relaxed)) {
                cell->data = item;  // Write data
                cell->sequence.store(pos + 1, std::memory_order_release);
                return true;  // Success!
            }
            // CAS failed, retry with updated pos
        } else if (diff < 0) {
            return false;  // Queue full
        } else {
            pos = enqueue_pos_.load(std::memory_order_relaxed);  // Reload
        }
    }
}
```

**Key Points**:
- **CAS (Compare-And-Swap)**: Atomic `enqueue_pos_` increment prevents data races
- **Sequence Numbers**: Track slot lifecycle (empty â†’ writing â†’ readable)
- **No ABA Problem**: Sequence monotonically increases
- **Latency**: ~20ns on Intel Skylake (5-10 CPU cycles with CAS contention)

#### Pop Operation (Single-Consumer)

**No Locks Needed** (only Dispatcher calls this):

```cpp
T* tryPop() {
    Cell* cell = &buffer_[dequeue_pos_ & mask_];
    uint64_t seq = cell->sequence.load(std::memory_order_acquire);
    intptr_t diff = (intptr_t)seq - (intptr_t)(dequeue_pos_ + 1);
    
    if (diff == 0) {  // Data ready
        T* data = &cell->data;
        cell->sequence.store(dequeue_pos_ + mask_ + 1,
                            std::memory_order_release);
        dequeue_pos_++;
        return data;  // ~15ns latency
    }
    
    return nullptr;  // Queue empty
}
```

#### Memory Ordering & Cache Coherence

**Critical for Correctness**:

| Operation | Memory Order | Purpose |
|-----------|--------------|---------|
| `enqueue_pos_.load()` | `relaxed` | No synchronization needed (CAS will retry) |
| `sequence.load()` | `acquire` | Ensure data visibility from producer |
| `enqueue_pos_.CAS()` | `relaxed` | CAS provides implicit ordering |
| `sequence.store()` | `release` | Make data visible to consumer |
| `dequeue_pos_++` | Plain write | Single-threaded, no atomics needed |

**Cache-Line Alignment**:

```cpp
struct alignas(64) Cell {  // 64 bytes = typical x86 cache line
    std::atomic<uint64_t> sequence;  // 8 bytes
    T data;                          // 8 bytes (pointer)
    char padding[48];                // Pad to 64 bytes
};
```

**Why?** Prevents **false sharing** â€” if two cells share a cache line, writes by different CPUs invalidate each other's caches (severe performance penalty).

#### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| Capacity | 65536 | Power of 2 for fast modulo |
| Push latency | 20ns (P50) | With CAS contention |
| Pop latency | 15ns (P50) | Single-threaded, no contention |
| Memory | 512KB | 65536 Ã— 8 bytes |
| Cache-line aligned | Yes | Prevents false sharing |
| ABA-safe | Yes | Sequence numbers prevent reuse issues |

---

### SPSC Queue (Single-Producer Single-Consumer)

**Implementation**: Custom atomic ring buffer  
**Code**: [include/utils/SpscRingBuffer.hpp](../../include/utils/SpscRingBuffer.hpp)

#### Why SPSC Instead of MPSC?

**Use Case**: Dispatcher â†’ Worker queues (1-to-1 relationship)

- **Simpler than MPSC**: No CAS needed, just atomic increments
- **Faster**: ~8ns vs ~20ns (no CAS contention)
- **More predictable**: Single-threaded producer/consumer = no retries

#### Data Structure

```cpp
template<typename T, size_t Capacity>
class SpscRingBuffer {
private:
    alignas(64) std::atomic<size_t> write_pos_{0};  // Producer cache line
    alignas(64) std::atomic<size_t> read_pos_{0};   // Consumer cache line (separate!)
    
    std::array<T, Capacity> buffer_;  // 16384 slots
    static constexpr size_t mask_ = Capacity - 1;
};
```

**Critical**: `write_pos_` and `read_pos_` on **separate cache lines** to avoid false sharing.

#### Push Operation (Dispatcher)

```cpp
bool push(const T& item) {
    size_t current_write = write_pos_.load(std::memory_order_relaxed);
    size_t next_write = (current_write + 1) & mask_;  // Fast modulo
    
    size_t current_read = read_pos_.load(std::memory_order_acquire);
    
    if (next_write == current_read) {
        return false;  // Queue full
    }
    
    buffer_[current_write] = item;  // Write data
    write_pos_.store(next_write, std::memory_order_release);  // Publish
    return true;  // ~8ns latency
}
```

#### Pop Operation (Worker Thread)

```cpp
bool pop(T& item) {
    size_t current_read = read_pos_.load(std::memory_order_relaxed);
    size_t current_write = write_pos_.load(std::memory_order_acquire);
    
    if (current_read == current_write) {
        return false;  // Queue empty
    }
    
    item = buffer_[current_read];  // Read data
    read_pos_.store((current_read + 1) & mask_, std::memory_order_release);
    return true;  // ~6ns latency
}
```

#### Memory Ordering

| Operation | Memory Order | Rationale |
|-----------|--------------|-----------|
| `write_pos_.load()` | `relaxed` | Producer only reads its own counter |
| `read_pos_.load()` (in push) | `acquire` | Check if consumer made progress |
| `write_pos_.store()` | `release` | Publish data to consumer |
| `write_pos_.load()` (in pop) | `acquire` | Check if producer wrote new data |
| `read_pos_.store()` | `release` | Signal slot available to producer |

**Why `acquire/release`?**  
Ensures **happens-before** relationship: producer's write to `buffer_[i]` is visible to consumer before consumer reads it.

#### Performance vs MPSC

| Queue Type | Push Latency | Pop Latency | Use Case |
|------------|--------------|-------------|----------|
| MPSC | ~20ns | ~15ns | N ingest threads â†’ 1 Dispatcher |
| SPSC | ~8ns | ~6ns | 1 Dispatcher â†’ N workers (N queues) |

**Total savings per event**: 12ns push + 9ns pop = **21ns faster** (60% improvement)

---

### Lock-Free Deduplicator

**Implementation**: Atomic hash map with CAS insertion  
**Code**: [include/utils/lock_free_dedup.hpp](../../include/utils/lock_free_dedup.hpp)

#### Purpose

**Idempotent event processing**: Ensure events processed exactly once, even with retries/network duplicates.

**Traditional Approach** (slow):

```cpp
std::mutex mtx;
std::unordered_map<uint32_t, uint64_t> seen_events;

bool is_duplicate(uint32_t id, uint64_t now_ms) {
    std::lock_guard lock(mtx);  // âŒ Lock contention!
    if (seen_events.count(id)) return true;
    seen_events[id] = now_ms;
    return false;
}
```

**Problem**: Mutex lock in hot path (~100ns overhead + contention).

#### Lock-Free Design

**Hash Table with Atomic Buckets**:

```cpp
class LockFreeDeduplicator {
private:
    struct Entry {
        uint32_t id;
        uint64_t timestamp_ms;
        Entry* next;  // Linked list for collisions
    };
    
    std::vector<std::atomic<Entry*>> buckets_;  // 4096 buckets
    static constexpr size_t NUM_BUCKETS = 4096;
};
```

#### Duplicate Check (Lock-Free Read Path)

```cpp
bool is_duplicate(uint32_t event_id, uint64_t now_ms) {
    size_t bucket_idx = event_id % NUM_BUCKETS;
    Entry* entry = buckets_[bucket_idx].load(std::memory_order_acquire);
    
    // Walk linked list (read-only, no locks!)
    while (entry) {
        if (entry->id == event_id) {
            if (now_ms - entry->timestamp_ms < IDEMPOTENT_WINDOW_MS) {
                return true;  // Duplicate! (within 1-hour window)
            }
            return false;  // Expired, treat as new
        }
        entry = entry->next;
    }
    
    return false;  // Not found, new event
}
```

**Latency**: ~30ns (hash + linked list walk)

#### Insert (CAS-Based)

```cpp
void mark_seen(uint32_t event_id, uint64_t timestamp_ms) {
    size_t bucket_idx = event_id % NUM_BUCKETS;
    Entry* new_entry = allocate_entry(event_id, timestamp_ms);
    
    Entry* old_head = buckets_[bucket_idx].load(std::memory_order_acquire);
    
    do {
        new_entry->next = old_head;
    } while (!buckets_[bucket_idx].compare_exchange_weak(
                old_head, new_entry,
                std::memory_order_release, std::memory_order_acquire));
}
```

**CAS ensures**: Only one thread successfully inserts at head (others retry).

#### Memory Management

**Problem**: When to free old entries?

**Solution**: Separate cleanup thread evicts entries older than 1 hour:

```cpp
void cleanup_thread() {
    while (running_) {
        std::this_thread::sleep_for(std::chrono::minutes(10));
        
        uint64_t now_ms = get_timestamp_ms();
        
        for (auto& bucket : buckets_) {
            Entry* head = bucket.load(std::memory_order_acquire);
            Entry* new_head = filter_expired(head, now_ms);
            bucket.store(new_head, std::memory_order_release);
        }
    }
}
```

**Trade-off**: Stale entries linger for up to 10 minutes, but hot path stays lock-free.

#### Performance vs Mutex

| Approach | Read Latency | Write Latency | Contention Impact |
|----------|--------------|---------------|-------------------|
| `std::mutex + unordered_map` | ~150ns | ~200ns | Serializes all threads |
| Lock-free deduplicator | ~30ns | ~80ns (CAS) | No blocking, retry on CAS failure |

**Speedup**: **5x faster** on read-heavy workload (99% reads, 1% writes).

---

## NUMA-Aware Threading

### NUMA Architecture

Modern servers have **multiple NUMA nodes** (CPU + local RAM). Accessing remote RAM incurs 2-3x latency penalty.

**Example**: AMD EPYC 7742
- 2 NUMA nodes (Node 0: cores 0-63, Node 1: cores 64-127)
- Local RAM: ~80ns latency
- Remote RAM: ~180ns latency

### Thread Binding Strategy

**Goal**: Pin threads and allocate memory on same NUMA node.

**Implementation**:

```cpp
// ProcessManager.cpp
void ProcessManager::bindWorkerToNUMA(int workerId) {
    int numaNode = workerId % numNUMANodes;
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    
    // Bind to cores on target NUMA node
    for (int core : getCoresForNUMA(numaNode)) {
        CPU_SET(core, &cpuset);
    }
    
    pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
    numa_set_preferred(numaNode);  // Allocate on this node
}
```

**Code Location**: [src/process/ProcessManager.cpp](../../src/process/ProcessManager.cpp)

### Memory Allocation

**IngestEventPool** allocates memory on the same NUMA node as the thread:

```cpp
// IngestEventPool.cpp
void IngestEventPool::bindToNUMA() {
    int numaNode = getCurrentNUMANode();
    numa_set_preferred(numaNode);
    
    // All subsequent malloc/new will use local RAM
    pool_.reserve(kPoolSize);
}
```

**Benefits**:
- 40-50% reduction in memory latency
- Better cache utilization
- Predictable performance across cores

---

## Frame Protocol

### Binary Format

EventStreamCore uses a **length-prefixed binary protocol** for TCP/UDP ingestion.

**Frame Structure**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Length (4B) â”‚ Priority â”‚ TopicLen   â”‚ Topic   â”‚  Payload    â”‚
â”‚   uint32    â”‚  uint8   â”‚  uint16    â”‚ string  â”‚   binary    â”‚
â”‚  Big Endian â”‚   0-255  â”‚ Big Endian â”‚ UTF-8   â”‚   raw       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Field Details**:

| Field | Type | Size | Description |
|-------|------|------|-------------|
| Length | uint32 (BE) | 4 bytes | Total frame size (excluding length field) |
| Priority | uint8 | 1 byte | Event priority (0=low, 255=high) |
| TopicLen | uint16 (BE) | 2 bytes | Topic string length |
| Topic | string | Variable | Topic identifier (UTF-8) |
| Payload | bytes | Variable | Event data (application-defined) |

**Example Frame**:

```
Topic: "sensor/temp"
Payload: {temperature: 23.5, unit: "C"}

Binary:
00 00 00 1F              // Length = 31 bytes
05                       // Priority = 5
00 0B                    // TopicLen = 11
73 65 6E 73 6F 72 2F ... // "sensor/temp"
7B 74 65 6D 70 65 72 ... // JSON payload
```

### Parsing Implementation

**Code Location**: [src/ingest/frame_parser.cpp](../../src/ingest/frame_parser.cpp)

**Key Functions**:

```cpp
// Parse header only (4B length + 1B priority + 2B topic_len)
ParsedFrame parseFrameBody(const uint8_t* buffer, size_t bufferSize);

// Parse complete frame (header + topic + payload)
ParsedFrame parseFullFrame(const uint8_t* buffer, size_t bufferSize);
```

**Zero-Copy Design**:
- Parser returns **pointers** into original buffer
- No string copies or allocations
- Caller owns buffer lifetime

---

## Event Pools

### IngestEventPool

**Purpose**: Pre-allocate event objects to avoid malloc/free in hot path.

**Architecture**:

```cpp
class IngestEventPool {
    // Thread-local storage
    static thread_local std::vector<IngestEvent*> pool_;
    static thread_local std::vector<IngestEvent*> activeEvents_;
    
    static constexpr size_t kPoolSize = 1000;  // Per thread
};
```

**Lifecycle**:

1. **Initialization** (thread startup):
   ```cpp
   IngestEventPool::bindToNUMA();  // Allocate 1000 events on local NUMA node
   ```

2. **Acquire** (per incoming frame):
   ```cpp
   IngestEvent* evt = IngestEventPool::acquireEvent();
   evt->topic = "sensor/temp";
   evt->payload = /* ... */;
   ```

3. **Release** (after processing):
   ```cpp
   IngestEventPool::releaseEvent(evt);  // Return to pool
   ```

**Benefits**:
- **Zero malloc** in steady state
- **NUMA-local** memory
- **Cache-friendly** sequential access

**Code Location**: [include/ingest/IngestEventPool.hpp](../../include/ingest/IngestEventPool.hpp)

---

## Topic-Based Routing

### TopicTable

**Purpose**: Map topic strings to handler functions.

**Implementation**:

```cpp
class TopicTable {
    std::unordered_map<std::string, HandlerFunc> handlers_;
    mutable std::shared_mutex mutex_;  // Reader-writer lock
    
public:
    void subscribe(const std::string& topic, HandlerFunc handler);
    HandlerFunc getHandler(const std::string& topic) const;
};
```

**Code Location**: [include/routing/TopicTable.hpp](../../include/routing/TopicTable.hpp)

### Dispatcher Integration

**Routing Logic**:

```cpp
void Dispatcher::processEvent(IngestEvent* evt) {
    HandlerFunc handler = topicTable_.getHandler(evt->topic);
    
    if (handler) {
        handler(evt);  // Invoke registered handler
    } else {
        logger_->warn("Unknown topic: {}", evt->topic);
    }
    
    IngestEventPool::releaseEvent(evt);
}
```

**Performance**:
- **O(1) lookup** via hash table
- **Read-heavy workload**: Shared mutex allows concurrent reads
- **Hot topics**: Hash collisions minimized with good hash function

---

## Performance Characteristics

### Throughput

| Workload | Events/sec | CPU Cores | Latency P99 |
|----------|-----------|-----------|-------------|
| TCP (1KB payload) | 5.2M | 32 | 4.2 Âµs |
| UDP (512B payload) | 8.5M | 32 | 2.8 Âµs |
| Mixed (50/50 TCP/UDP) | 6.8M | 32 | 3.5 Âµs |

### Memory Usage

| Component | Memory/Event | Notes |
|-----------|--------------|-------|
| IngestEvent | 256 bytes | Pool pre-allocation |
| MPSC Queue Slot | 8 bytes | Pointer only |
| SPSC Queue Slot | 8 bytes | Pointer only |
| Total per Event | ~280 bytes | Includes metadata |

### Latency Breakdown

**End-to-End Latency** (TCP ingestion â†’ worker processing):

| Stage | Time | % of Total |
|-------|------|------------|
| TCP recv() | 400 ns | 28% |
| Frame parsing | 180 ns | 13% |
| MPSC push | 120 ns | 8% |
| Dispatcher routing | 200 ns | 14% |
| SPSC push | 80 ns | 6% |
| Worker processing | 450 ns | 31% |
| **Total** | **1.43 Âµs** | **100%** |

### CPU Efficiency

**NUMA Impact**:

| Configuration | Throughput | Latency P99 |
|---------------|-----------|-------------|
| No NUMA binding | 4.8M/s | 12.5 Âµs |
| NUMA-aware (local) | 6.8M/s | 3.5 Âµs |
| **Improvement** | **+42%** | **-72%** |

---

## Next Steps

- **[Distributed System Plans](../distributed/)**: Migration to multi-node architecture
- **[Microservices Integration](../microservices/)**: REST API and monitoring services
- **[Build & Test Guide](../../README.md#quick-start)**: Compilation and benchmarking

---

*Last updated: January 2026*
